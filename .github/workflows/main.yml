# create a workflow that runs scrape.py once every hour
name: Main

# Trigger the workflow every hour, manually or when pushing to the main branch
on:
  workflow_dispatch:

  schedule:
    - cron: "0 * * * *"

# also support manual triggering


jobs:
  update-data:
    runs-on: ubuntu-latest
    steps:
      # Checkout the repository so that we have access to the Python script
      - name: Check out the code
        uses: actions/checkout@v3

      # Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'  # Specify your required Python version

      # Install dependencies (if any)
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # Run the Python script
      - name: Run cron.py
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GEOCODING_BASE_URL: ${{ vars.GEOCODING_BASE_URL }}
          DRIVEBC_MAJOR_EVENTS_URL: ${{ vars.DRIVEBC_MAJOR_EVENTS_URL }}
          DOCUMENTDB_PASSWORD: ${{ secrets.DOCUMENTDB_PASSWORD }}
          DOCUMENTDB_USER: ${{ secrets.DOCUMENTDB_USER }}
          DOCUMENTDB_DB_NAME: ${{ secrets.DOCUMENTDB_DB_NAME }}
          DOCUMENTDB_COLLECTION_NAME: ${{ secrets.DOCUMENTDB_COLLECTION_NAME }}
        run: python cron.py
      # update data.json
      - name: Update resources
        uses: test-room-7/action-update-file@v1
        with:
            file-path: src/data/data.json
            commit-msg: update data.json
            github-token: ${{ secrets.GH_TOKEN }}
